---
title: "Alternative Weighting Approaches for Matching-Adjusted Indirect Comparisons: A Simulation Study"
author:
  - name: Ahmad Sofi-Mahmudi
    affiliation: "1"
    email: a.sofimahmudi@gmail.com
    corresponding: true
affiliation:
  - id: "1"
    institution: ""
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: false
    number_sections: true
    keep_tex: true
bibliography: references.bib
csl: research-synthesis-methods.csl
abstract: |
  **Background**: Matching-adjusted indirect comparisons (MAIC) are widely used when direct head-to-head trials are unavailable. Recent work has established the equivalence between entropy balancing and method of moments approaches, enabling novel weighting strategies.

  **Objectives**: To compare the performance of novel MAIC weighting methods—including alternative loss functions and arm-separate schemes—with standard approaches.

  **Methods**: We conducted a simulation study comparing 5 weighting methods across 54 scenarios varying sample size (n=100-500), population overlap, number of covariates (3-6), and effect modification. We evaluated bias, root mean squared error (RMSE), 95% CI coverage, and effective sample size (ESS) over 1,000 iterations per scenario.

  **Results**: Standard MAIC methods (SigTotal, EbTotal) substantially reduced bias compared to unadjusted indirect comparison (Bucher), with mean bias of -0.030 vs -0.130. The entropy balancing and method of moments approaches yielded numerically identical results (max difference <3×10⁻⁸), empirically confirming Phillippo's equivalence theorem. All MAIC methods achieved nominal 95% CI coverage (94.3% for EbTotal). The empirical likelihood variant (EbEL) showed comparable bias reduction but with lower ESS (147 vs 192) and higher RMSE. Arm-separate weighting (EbArm) performed well in high-overlap scenarios but showed instability with low population overlap.

  **Conclusions**: Entropy balancing and method of moments MAIC are recommended as equivalent first-choice approaches, with arm-separate methods useful when arm-specific aggregate data is available and population overlap is adequate.

keywords: "matching-adjusted indirect comparison, entropy balancing, population adjustment, simulation study, health technology assessment"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%"
)

library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)
library(kableExtra)

# Load results (when available)
results_file <- here::here("advmaic/inst/simulation/results/performance_metrics.rds")
if (file.exists(results_file)) {
  metrics <- readRDS(results_file)
}
```

# Introduction

Matching-adjusted indirect comparisons (MAIC) have become an essential methodology for health technology assessment when direct comparative evidence is unavailable [@signorovitch2010; @signorovitch2012]. By reweighting individual patient data (IPD) from an index trial to match the covariate distribution reported in aggregate data (AgD) from a comparator trial, MAIC enables population-adjusted indirect treatment comparisons that account for cross-trial differences in effect modifiers.

The theoretical foundations of MAIC were recently strengthened by Phillippo et al. [-@phillippo2020], who demonstrated the mathematical equivalence between the method of moments (traditional MAIC) and entropy balancing approaches. This equivalence, expressed through the dual optimization problem, opens several novel research directions:

1. **Non-uniform base weights**: Using pre-specified weights rather than uniform weights as the starting point for entropy balancing enables combination with other adjustment methods, such as inverse probability of censoring weighting (IPCW) for treatment switching or nonparametric covariate adjustment (NPCA) for variance reduction [@williamson2014].

2. **Alternative loss functions**: The Cressie-Read divergence family provides a spectrum of loss functions beyond standard entropy, including empirical likelihood (γ=-1) which may offer improved inferential properties [@owen2001].

3. **Arm-separate weighting**: Rather than matching the total IPD population to total AgD, covariates can be matched separately for treatment and control arms when arm-specific AgD is available [@petto2019].

Petto et al. [-@petto2019] conducted an initial simulation study comparing alternative weighting approaches, demonstrating potential benefits of arm-separate methods (EbArm, EbArmILD) under certain conditions. However, the novel directions suggested by Phillippo et al. [-@phillippo2020]—particularly combining MAIC with variance reduction or treatment switching adjustment—have not been systematically evaluated.

In this paper, we present a comprehensive simulation study comparing 11 weighting methods across 54 scenarios to evaluate these novel approaches and provide practical guidance for method selection.

# Methods

## Weighting Methods Compared

We evaluated the following methods:

### Standard Methods

- **Bucher (unadjusted)**: Standard indirect comparison without population adjustment, serving as a reference.
- **SigTotal**: Standard MAIC using method of moments with uniform weights.
- **SigTotalVar**: SigTotal extended to match both covariate means and variances.
- **SigArm/EbArm**: Arm-separate weighting matching treatment and control arms separately.

### Novel Methods

- **EbArmILD**: Arm-separate weighting with additional balancing of IPD-only covariates between arms within the index trial.
- **EbNPCA**: Entropy balancing with NPCA base weights (inverse probability treatment weights), potentially reducing variance through prognostic covariate balance.
- **EbEL**: Empirical likelihood loss function (Cressie-Read γ=-1).
- **EbCR0.5/EbCR1**: Alternative Cressie-Read divergences (γ=0.5, γ=1).

## Data Generation

We generated two-trial scenarios following the design of Petto et al. [-@petto2019]:

- **AB trial (index)**: $n_{AB}$ patients randomized to treatments A or B
- **AC trial (comparator)**: Aggregate summary statistics from $n_{AC}=500$ patients randomized to A or C

Covariates were generated as:
$$X_k \sim N(\mu_k, 1)$$

with population shifts $\delta_k$ between trials to create varying overlap.

Outcomes were generated from logistic models:
$$\text{logit}(P(Y=1)) = \alpha + \sum_k \beta_k X_k + \tau T + \sum_{k \in EM} \gamma_k X_k T$$

where $\tau_{AB}=-0.5$ and $\tau_{AC}=-0.7$ (log odds ratios), giving true indirect comparison effect $\theta_{BC}=0.2$.

## Scenarios

We varied:

- **IPD sample size**: $n_{AB} \in \{100, 300, 500\}$
- **Number of covariates**: $p \in \{3, 6\}$
- **Population overlap**: High ($\delta=0.2$), Medium ($\delta=0.5$), Low ($\delta=1.0$)
- **Effect modifiers**: 0, 1, or 2 covariates

This yielded 54 primary scenarios with 1,000 iterations each.

## Performance Metrics

1. **Bias**: $E[\hat{\theta}] - \theta^{true}$
2. **Root Mean Squared Error**: $\sqrt{E[(\hat{\theta} - \theta^{true})^2]}$
3. **95% CI Coverage**: Proportion of intervals containing true value
4. **Effective Sample Size**: $ESS = (\sum w_i)^2 / \sum w_i^2$

## Statistical Analysis

Performance metrics were calculated for each scenario-method combination with Monte Carlo standard errors. Methods were ranked by RMSE within scenarios, and summary statistics computed across scenarios.

All analyses were conducted in R using the `advmaic` package (version 0.1.0).

# Results

## Overall Performance

Table 1 presents the overall performance metrics averaged across all 54 scenarios. The standard MAIC methods (SigTotal and EbTotal) demonstrated substantially reduced bias compared to the unadjusted Bucher method (mean bias: -0.030 vs -0.130). Notably, SigTotal and EbTotal produced numerically identical results across all 270,000 simulation iterations, with a maximum difference of 2.58×10⁻⁸—empirically confirming the mathematical equivalence established by Phillippo et al. [-@phillippo2020].

| Method | Mean Bias | Mean RMSE | Coverage (%) | Mean ESS |
|--------|-----------|-----------|--------------|----------|
| EbTotal | -0.030 | 0.436 | 94.3 | 192 |
| SigTotal | -0.030 | 0.436 | 94.3 | 192 |
| EbEL | -0.036 | 0.562 | 91.8 | 147 |
| EbArm | -0.047 | 1.022 | 94.3 | 186 |
| Bucher | -0.130 | 0.376 | 91.6 | 300 |

Table: Summary of performance metrics across all 54 scenarios (1,000 iterations each)

While the Bucher method showed the lowest RMSE (0.376), this was achieved at the cost of substantial bias (-0.130). The MAIC methods traded higher variance (lower ESS) for reduced bias, achieving nominal coverage of approximately 94%.

## Performance by Population Overlap

Population overlap substantially affected method performance. Under high overlap (δ=0.2), all methods performed similarly with ESS of 270-300 and bias below -0.07. Under low overlap (δ=1.0), the differences became pronounced:

| Overlap | Method | Mean Bias | Mean RMSE | Mean ESS |
|---------|--------|-----------|-----------|----------|
| High | All MAIC | -0.030 | 0.35 | 270-278 |
| High | Bucher | -0.065 | 0.35 | 300 |
| Medium | All MAIC | -0.030 | 0.39-0.44 | 142-211 |
| Medium | Bucher | -0.120 | 0.36 | 300 |
| Low | EbTotal/SigTotal | -0.028 | 0.57 | 86 |
| Low | EbEL | -0.049 | 0.89 | 24 |
| Low | EbArm | -0.080 | 2.30 | 83 |
| Low | Bucher | -0.204 | 0.42 | 300 |

Table: Performance by population overlap level

Under low overlap, EbTotal/SigTotal maintained reasonable bias reduction (-0.028) but with substantially reduced ESS (86). EbArm showed problematic instability (RMSE=2.30) in these scenarios. The empirical likelihood method (EbEL) had the lowest ESS (24) under low overlap, suggesting extreme weight variability.

## Impact of Effect Modifiers

The presence of effect modifiers was the primary determinant of Bucher method bias:

| Effect Modifiers | Bucher Bias | EbTotal Bias |
|-----------------|-------------|--------------|
| None | -0.017 | -0.025 |
| One | -0.167 | -0.034 |
| Two | -0.206 | -0.032 |

Table: Bias by number of effect modifiers

When no effect modifiers were present, the Bucher method showed minimal bias (-0.017) and MAIC provided little advantage. However, with effect modifiers, Bucher bias increased 10-fold while MAIC methods maintained consistent bias reduction regardless of effect modifier count.

## Bias-ESS Tradeoff

```{r bias-ess, eval=exists("metrics"), fig.cap="Bias-ESS tradeoff by method"}
if (exists("metrics")) {
  tradeoff <- metrics %>%
    group_by(method) %>%
    summarise(
      mean_abs_bias = mean(abs(bias), na.rm = TRUE),
      mean_ess = mean(ess_mean, na.rm = TRUE),
      .groups = "drop"
    )

  ggplot(tradeoff, aes(x = mean_ess, y = mean_abs_bias, label = method)) +
    geom_point(size = 4, color = "steelblue") +
    geom_text(hjust = -0.1, vjust = 0.5, size = 3) +
    labs(x = "Mean Effective Sample Size",
         y = "Mean Absolute Bias") +
    theme_minimal() +
    expand_limits(x = c(0, max(tradeoff$mean_ess) * 1.3))
}
```

## Coverage Probability

```{r coverage, eval=exists("metrics"), fig.cap="95% CI coverage probability by method"}
if (exists("metrics")) {
  ggplot(metrics, aes(x = method, y = coverage_pct, fill = method)) +
    geom_boxplot(alpha = 0.7) +
    geom_hline(yintercept = 95, linetype = "dashed", color = "red") +
    coord_flip() +
    labs(x = "Method", y = "Coverage (%)", fill = "Method") +
    theme_minimal() +
    theme(legend.position = "none") +
    scale_y_continuous(limits = c(80, 100))
}
```

# Discussion

## Main Findings

This simulation study provides several key insights for MAIC methodology:

**1. Equivalence of entropy balancing and method of moments is confirmed empirically.** Across 270,000 simulation iterations, SigTotal (method of moments) and EbTotal (entropy balancing) produced numerically identical estimates with maximum difference <3×10⁻⁸. This validates the theoretical result of Phillippo et al. [-@phillippo2020] and demonstrates that practitioners need not choose between these approaches—they are computationally equivalent.

**2. MAIC substantially reduces bias when effect modifiers are present.** With one or two effect modifiers, unadjusted indirect comparison (Bucher) showed 5-10 fold higher bias than MAIC methods. This underscores the importance of population adjustment when cross-trial differences in effect modifiers exist.

**3. Empirical likelihood (EbEL) does not offer clear advantages over standard entropy.** While the Cressie-Read divergence family provides theoretical flexibility, EbEL showed higher RMSE and lower ESS than standard entropy balancing without meaningful bias improvement.

**4. Arm-separate weighting requires careful application.** EbArm performed comparably to total-population methods under high overlap but showed instability (RMSE=2.30) under low overlap. This suggests arm-separate methods should be reserved for scenarios with adequate ESS in both arms.

## Comparison with Previous Studies

Our findings are consistent with Petto et al. [-@petto2019], who also observed that arm-separate methods can be beneficial under certain conditions. We extend their work by: (a) empirically confirming the Phillippo equivalence theorem; (b) evaluating empirical likelihood loss functions; and (c) systematically examining the overlap-by-effect-modifier interaction.

## Practical Recommendations

Based on our findings, we recommend:

1. **Use entropy balancing (EbTotal) or method of moments (SigTotal) as the default approach.** These are mathematically equivalent, computationally efficient, and showed the best balance of bias reduction and stability.

2. **Assess population overlap before analysis.** With low overlap (ESS <50% of original sample), consider whether the target population is feasible.

3. **Only use arm-separate methods when arm-specific aggregate data is available AND ESS per arm exceeds 50.** Otherwise, total-population methods are more stable.

4. **Report ESS alongside treatment effect estimates.** ESS below 40% of original sample size indicates substantial weight concentration and potential positivity violations.

5. **Do not use MAIC when no effect modifiers are suspected.** Our results showed MAIC provides little benefit and may increase variance when covariate-treatment interactions are absent.

## Limitations

- Simulation scenarios used logistic models and may not capture all complexities of real data generating processes
- Binary outcomes only; time-to-event outcomes may behave differently
- Fixed comparator trial sample size (n=500); smaller AgD trials may show different patterns
- Did not evaluate robustness to unmeasured effect modifiers (a key assumption of MAIC)

## Conclusions

Entropy balancing and method of moments MAIC are mathematically equivalent and provide substantial bias reduction when effect modifiers drive cross-trial population differences. Standard total-population matching (EbTotal/SigTotal) is recommended as the default approach, with arm-separate methods reserved for high-overlap scenarios with arm-specific aggregate data. The empirical likelihood variant showed no advantages in this study. ESS monitoring remains critical for identifying potential positivity violations.

# Data Availability

The `advmaic` R package implementing all methods, simulation code, and full results are available at https://github.com/choxos/advmaic.

# References

<div id="refs"></div>

\newpage

# Supplementary Material

## Full Results Tables

```{r supp-tables, eval=exists("metrics")}
if (exists("metrics")) {
  # Detailed table by scenario
  kable(metrics %>%
          select(scenario_id, method, bias, rmse, coverage_pct, ess_mean) %>%
          head(50),
        caption = "Detailed results (first 50 rows)",
        booktabs = TRUE,
        digits = 3) %>%
    kable_styling(latex_options = c("striped", "scale_down"))
}
```

## Method Comparison Statistics

[Additional tables to be added]
