% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Alternative Weighting Approaches for Matching-Adjusted Indirect Comparisons: A Simulation Study},
  pdfkeywords={matching-adjusted indirect comparison, entropy balancing,
population adjustment, simulation study, health technology assessment,
indirect treatment comparison},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Alternative Weighting Approaches for Matching-Adjusted Indirect
Comparisons: A Simulation Study}
\author{true}
\date{January 2026}

\begin{document}
\maketitle
\begin{abstract}
\textbf{Background}: Matching-adjusted indirect comparisons (MAIC)
enable treatment comparisons when direct head-to-head trials are
unavailable by reweighting individual patient data to match aggregate
covariate distributions from comparator trials. Recent theoretical work
established the mathematical equivalence between the original method of
moments approach and entropy balancing, opening possibilities for
alternative weighting strategies including non-uniform base weights,
alternative loss functions from the Cressie-Read divergence family, and
arm-separate weighting schemes.

\textbf{Objectives}: This simulation study compared the statistical
performance of alternative MAIC weighting methods across scenarios
varying in sample size, population overlap, number of covariates, and
effect modification to provide evidence-based guidance for method
selection in practice.

\textbf{Methods}: We conducted a comprehensive simulation study
comparing five weighting methods (Bucher unadjusted comparison, method
of moments MAIC, entropy balancing MAIC, empirical likelihood MAIC, and
arm-separate entropy balancing) across 54 scenarios with 1,000
iterations each. Performance was evaluated using bias, root mean squared
error, 95\% confidence interval coverage, and effective sample size.

\textbf{Results}: The method of moments (SigTotal) and entropy balancing
(EbTotal) approaches produced numerically identical estimates across all
270,000 simulation iterations, with maximum difference below 3×10⁻⁸,
empirically confirming the theoretical equivalence theorem. All MAIC
methods substantially reduced bias compared to unadjusted indirect
comparison when effect modifiers were present, with mean bias of -0.030
versus -0.130 for the Bucher method. Coverage probability was
approximately 94\% for entropy balancing methods. The empirical
likelihood variant showed higher root mean squared error and lower
effective sample size without meaningful bias improvement. Arm-separate
weighting performed well under high population overlap but exhibited
instability under low overlap conditions.

\textbf{Conclusions}: Entropy balancing and method of moments MAIC are
mathematically equivalent and recommended as first-choice approaches for
population-adjusted indirect comparisons. Arm-separate methods should be
reserved for scenarios with adequate effective sample size per arm.
Alternative loss functions from the Cressie-Read family did not
demonstrate advantages over standard entropy balancing in this study.
\end{abstract}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\newpage

\section{Introduction}\label{introduction}

Comparative effectiveness research forms the foundation of
evidence-based health technology assessment and clinical
decision-making. Ideally, treatment comparisons would be based on
head-to-head randomized controlled trials that directly compare all
interventions of interest under identical conditions. In practice,
however, such comprehensive trial programs are rarely available, leaving
decision-makers to draw inferences from indirect evidence.

The indirect treatment comparison methodology developed by Bucher and
colleagues in 1997 (Bucher et al. 1997) provides a framework for
comparing treatments that have not been directly compared in clinical
trials. By leveraging a common comparator treatment, the Bucher method
computes the relative effect between two treatments as the difference in
their effects relative to the shared comparator. This approach assumes
that the relative treatment effects are consistent across trial
populations, an assumption known as transitivity or similarity. When
patient populations differ systematically between trials, particularly
in characteristics that modify treatment effects, the transitivity
assumption may be violated, leading to biased indirect comparisons.

Matching-adjusted indirect comparison (MAIC), introduced by Signorovitch
and colleagues (Signorovitch et al. 2010, 2012), addresses this
limitation through propensity score-like reweighting. When individual
patient data (IPD) are available from one trial and only aggregate
summary statistics are available from a comparator trial, MAIC reweights
the IPD to create a pseudo-population matching the covariate
distribution of the aggregate data population. This population
adjustment aims to remove confounding due to cross-trial differences in
effect modifiers, enabling more valid indirect comparisons.

The original MAIC method estimates weights using the method of moments,
solving a system of equations that equates weighted covariate means in
the IPD to the published aggregate data means. Phillippo and colleagues
(Phillippo et al. 2020) recently established a fundamental theoretical
result demonstrating that this method of moments approach is
mathematically equivalent to entropy balancing, an approach from the
causal inference literature that minimizes the Kullback-Leibler
divergence from uniform weights subject to covariate balance
constraints. This equivalence, proven through convex optimization
duality, has important implications for MAIC methodology.

The entropy balancing formulation naturally accommodates generalizations
that the original method of moments specification does not readily
express. Non-uniform base weights can be incorporated, enabling the
combination of MAIC with other weighting-based adjustments such as
inverse probability of censoring weighting for treatment switching or
inverse probability of treatment weighting for variance reduction
(Williamson et al. 2014). Alternative loss functions from the
Cressie-Read divergence family can be substituted for entropy,
potentially offering different statistical properties (Cressie and Read
1984; Owen 2001). Arm-separate weighting schemes that match treatment
and control arms separately to arm-specific aggregate data have also
been proposed (Petto et al. 2019).

Despite these methodological developments, the relative performance of
alternative MAIC weighting approaches has not been comprehensively
evaluated. This simulation study addresses this gap by comparing the
statistical performance of standard MAIC methods and their extensions
across a range of data-generating scenarios relevant to health
technology assessment applications.

\section{Methods}\label{methods}

\subsection{Matching-Adjusted Indirect Comparison
Framework}\label{matching-adjusted-indirect-comparison-framework}

The MAIC framework addresses the comparison of treatments B and C when
no direct comparative trial exists, but IPD are available from a trial
comparing A versus B (the index trial) and aggregate data are available
from a published trial comparing A versus C (the comparator trial). Let
\(\{(X_i, T_i, Y_i)\}_{i=1}^n\) denote the IPD, where
\(X_i \in \mathbb{R}^p\) is a vector of baseline covariates,
\(T_i \in \{0,1\}\) indicates treatment assignment (0 for A, 1 for B),
and \(Y_i\) is the outcome. Let \(\bar{X}^{AC}\) denote the published
covariate means from the comparator trial.

The goal is to estimate the indirect comparison
\(\theta_{BC} = d_{AB} - d_{AC}\), where \(d_{AB}\) and \(d_{AC}\) are
the relative treatment effects in the respective trials. When the
covariate distribution in the index trial differs from that in the
comparator trial, direct use of the unadjusted \(d_{AB}\) may yield a
biased indirect comparison. MAIC addresses this by finding weights
\(w_i \geq 0\) such that the weighted covariate distribution in the IPD
matches the aggregate data distribution, then estimating a weighted
treatment effect.

\subsection{Method of Moments
Weighting}\label{method-of-moments-weighting}

The original MAIC approach derives weights by modeling the odds of
belonging to the comparator trial population given baseline covariates.
Assuming a logistic model for trial membership odds, the weights take
the form \(w_i \propto \exp(\alpha^T X_i)\) where \(\alpha\) is a
parameter vector. The parameter \(\alpha\) is estimated by solving the
moment conditions:

\[\sum_{i=1}^n w_i (X_i - \bar{X}^{AC}) = 0\]

with weights normalized to sum to one. In practice, this is equivalent
to minimizing the objective function:

\[Q(\alpha) = \log\left(\sum_{i=1}^n \exp(\alpha^T \tilde{X}_i)\right)\]

where \(\tilde{X}_i = X_i - \bar{X}^{AC}\) are centered covariates. The
gradient of this objective is precisely the weighted mean of centered
covariates, which equals zero at the optimum.

\subsection{Entropy Balancing
Weighting}\label{entropy-balancing-weighting}

Entropy balancing approaches the weight estimation problem from an
information-theoretic perspective. The goal is to find weights that are
as close as possible to a set of base weights while satisfying exact
covariate balance constraints. With uniform base weights
\(w_i^{(0)} = 1/n\), the entropy balancing problem is:

\[\min_{w} \sum_{i=1}^n w_i \log(n w_i)\]

subject to:

\[\sum_{i=1}^n w_i X_i = \bar{X}^{AC}, \quad \sum_{i=1}^n w_i = 1, \quad w_i \geq 0\]

The objective function is the Kullback-Leibler divergence from uniform
weights, also known as the negative entropy of the weight distribution.
Solving this constrained optimization problem using Lagrange multipliers
yields weights of the form:

\[w_i = \frac{\exp(\alpha^T X_i)}{\sum_{j=1}^n \exp(\alpha^T X_j)}\]

where \(\alpha\) is the vector of Lagrange multipliers for the balance
constraints. Phillippo et al. (Phillippo et al. 2020) proved that this
\(\alpha\) is identical to the \(\alpha\) from the method of moments,
establishing the mathematical equivalence of the two approaches.

\subsection{Cressie-Read Divergence Family and Empirical
Likelihood}\label{cressie-read-divergence-family-and-empirical-likelihood}

The entropy balancing objective can be generalized by considering
alternative divergence measures from the Cressie-Read family (Cressie
and Read 1984):

\[D_\gamma(w \| w^{(0)}) = \frac{1}{\gamma(\gamma+1)} \sum_{i=1}^n w_i^{(0)} \left[\left(\frac{w_i}{w_i^{(0)}}\right)^{\gamma+1} - 1\right]\]

The parameter \(\gamma\) indexes different divergence measures:
\(\gamma = 0\) corresponds to Kullback-Leibler divergence (entropy),
\(\gamma = -1\) yields empirical likelihood, \(\gamma = 1\) gives
chi-squared divergence, and \(\gamma = -0.5\) produces Hellinger
distance. The empirical likelihood case is particularly interesting
given its established asymptotic properties, including Bartlett
correctability and data-determined confidence region shapes (Owen 2001).

Minimizing the empirical likelihood divergence subject to the balance
constraints yields a different weight distribution than entropy
balancing, though both satisfy exact covariate balance. Whether the
theoretical advantages of empirical likelihood translate to practical
benefits in MAIC applications is an empirical question addressed by this
simulation study.

\subsection{Arm-Separate Weighting}\label{arm-separate-weighting}

Standard MAIC matches the total IPD population to the total aggregate
data population. When arm-specific aggregate data are available, an
alternative is to match covariates separately for treatment and control
arms (Petto et al. 2019). Let \(\bar{X}^{AC}_{T=1}\) and
\(\bar{X}^{AC}_{T=0}\) denote the arm-specific covariate means from the
comparator trial. The arm-separate approach solves two separate entropy
balancing problems:

For treated patients (\(T_i = 1\)): minimize divergence subject to
weighted means matching \(\bar{X}^{AC}_{T=1}\)

For control patients (\(T_i = 0\)): minimize divergence subject to
weighted means matching \(\bar{X}^{AC}_{T=0}\)

This approach may better preserve within-arm covariate balance,
particularly when random imbalance exists between arms in the index
trial.

\subsection{Simulation Design}\label{simulation-design}

\subsubsection{Data Generation}\label{data-generation}

Data were generated from a two-trial scenario. The index trial (AB)
provided IPD with \(n_{AB}\) patients randomized 1:1 to treatments A and
B. The comparator trial (AC) was represented only by aggregate
statistics derived from a simulated population of 500 patients.

Covariates were generated from multivariate normal distributions with
mean \(\mu_{AB} = \mathbf{0}\) for the index trial and
\(\mu_{AC} = \delta \cdot \mathbf{1}_p\) for the comparator trial, where
\(\delta\) controlled the population shift magnitude and
\(\mathbf{1}_p\) is a vector of ones. Covariance was set to the identity
matrix for both populations.

Binary outcomes were generated from logistic models incorporating
prognostic effects and effect modification:

\[\text{logit}(P(Y=1)) = -1 + 0.3 \sum_{k=1}^p X_k + \tau T + 0.2 \sum_{k \in EM} X_k T\]

where \(\tau_{AB} = -0.5\) for the index trial and \(\tau_{AC} = -0.7\)
for the comparator trial, yielding a true indirect comparison effect of
\(\theta_{BC} = 0.2\) on the log odds ratio scale.

\subsubsection{Scenarios}\label{scenarios}

The simulation varied four factors in a full factorial design: IPD
sample size (\(n_{AB} \in \{100, 300, 500\}\)), number of covariates
(\(p \in \{3, 6\}\)), population overlap (high: \(\delta = 0.2\);
medium: \(\delta = 0.5\); low: \(\delta = 1.0\)), and number of effect
modifiers (0, 1, or 2 covariates). This yielded 54 primary scenarios,
each replicated 1,000 times.

\subsubsection{Methods Compared}\label{methods-compared}

Five methods were evaluated: Bucher (unadjusted indirect comparison
serving as reference), SigTotal (method of moments MAIC with total
population matching), EbTotal (entropy balancing MAIC with total
population matching), EbArm (arm-separate entropy balancing), and EbEL
(empirical likelihood loss with total population matching).

\subsubsection{Treatment Effect
Estimation}\label{treatment-effect-estimation}

For each method, the weighted treatment effect was estimated using
logistic regression with the estimated weights. Robust (sandwich)
standard errors were computed to account for weight estimation
uncertainty. The indirect comparison was computed as the difference
between the weighted index trial effect and the aggregate data effect,
with variance computed assuming independence between trials.

\subsubsection{Performance Metrics}\label{performance-metrics}

Four metrics were computed for each method in each scenario: bias (mean
estimate minus true value), root mean squared error (RMSE), 95\%
confidence interval coverage probability, and effective sample size
(ESS), computed as \((\sum w_i)^2 / \sum w_i^2\).

\subsection{Software}\label{software}

All analyses were conducted in R (version 4.3). The simulation code and
weighting method implementations are available in the advmaic R package
at \url{https://github.com/choxos/advmaic}.

\section{Results}\label{results}

\subsection{Overall Method
Performance}\label{overall-method-performance}

Table 1 presents the overall performance metrics averaged across all 54
scenarios. The entropy balancing method (EbTotal) and method of moments
(SigTotal) produced identical results, with mean bias of -0.030,
coverage of 94.3\%, and mean effective sample size of 192. The
unadjusted Bucher method showed substantially higher bias (-0.130) but
the lowest RMSE (0.376) due to avoiding variance inflation from
weighting. The empirical likelihood method (EbEL) had higher RMSE
(0.562) and lower ESS (147) than standard entropy balancing without bias
improvement. The arm-separate method (EbArm) showed the highest RMSE
(1.022) driven by instability in challenging scenarios.

\begin{table}[h]
\centering
\caption{Overall performance metrics across all 54 scenarios (1,000 iterations each)}
\begin{tabular}{lcccc}
\hline
Method & Mean Bias & Mean RMSE & Coverage (\%) & Mean ESS \\
\hline
EbTotal & -0.030 & 0.436 & 94.3 & 192 \\
SigTotal & -0.030 & 0.436 & 94.3 & 192 \\
EbEL & -0.036 & 0.562 & 91.8 & 147 \\
EbArm & -0.047 & 1.022 & 94.3 & 186 \\
Bucher & -0.130 & 0.376 & 91.6 & 300 \\
\hline
\end{tabular}
\end{table}

\subsection{Equivalence of Method of Moments and Entropy
Balancing}\label{equivalence-of-method-of-moments-and-entropy-balancing}

A central finding of this study is the empirical confirmation of the
theoretical equivalence between method of moments (SigTotal) and entropy
balancing (EbTotal). Across all 270,000 simulation iterations (54
scenarios × 5 methods × 1,000 replications), the maximum absolute
difference between SigTotal and EbTotal point estimates was
\(2.58 \times 10^{-8}\), and the mean difference was
\(3.27 \times 10^{-11}\). These differences are attributable solely to
numerical precision in the optimization algorithms and confirm that the
two methods are computationally equivalent. Figure 1 displays a scatter
plot of EbTotal versus SigTotal estimates, showing perfect alignment
along the identity line.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-equivalence-1} 

}

\caption{Equivalence of method of moments (SigTotal) and entropy balancing (EbTotal). Each point represents one simulation iteration. The dashed red line is the identity line. The correlation is 1.0000000000 and maximum absolute difference is 2.58×10⁻⁸.}\label{fig:fig-equivalence}
\end{figure}

\subsection{Effect of Population
Overlap}\label{effect-of-population-overlap}

Population overlap had a substantial impact on method performance. Table
2 presents results stratified by overlap level. Under high overlap
(\(\delta = 0.2\)), all methods performed similarly, with ESS remaining
high (270-300) and bias modest for all methods including Bucher
(-0.065). Under low overlap (\(\delta = 1.0\)), the differences became
pronounced. The Bucher method showed bias of -0.204 while
EbTotal/SigTotal maintained low bias (-0.028) but with substantially
reduced ESS (86). The empirical likelihood method showed particularly
severe ESS reduction (24) under low overlap, suggesting extreme weight
variability. The arm-separate method exhibited instability under low
overlap, with RMSE of 2.30.

\begin{table}[h]
\centering
\caption{Performance by population overlap level}
\begin{tabular}{llcccc}
\hline
Overlap & Method & Mean Bias & Mean RMSE & Coverage (\%) & Mean ESS \\
\hline
High & EbTotal/SigTotal & -0.031 & 0.353 & 94.9 & 278 \\
High & EbArm & -0.030 & 0.340 & 95.9 & 270 \\
High & EbEL & -0.029 & 0.356 & 94.8 & 273 \\
High & Bucher & -0.065 & 0.346 & 94.5 & 300 \\
\hline
Medium & EbTotal/SigTotal & -0.032 & 0.388 & 94.6 & 211 \\
Medium & EbArm & -0.030 & 0.430 & 95.7 & 204 \\
Medium & EbEL & -0.030 & 0.442 & 93.9 & 142 \\
Medium & Bucher & -0.120 & 0.364 & 92.7 & 300 \\
\hline
Low & EbTotal/SigTotal & -0.028 & 0.569 & 93.5 & 86 \\
Low & EbArm & -0.080 & 2.296 & 91.3 & 83 \\
Low & EbEL & -0.049 & 0.887 & 86.6 & 24 \\
Low & Bucher & -0.204 & 0.417 & 87.6 & 300 \\
\hline
\end{tabular}
\end{table}

Figure 2 displays the distribution of bias across all scenarios
stratified by population overlap level, illustrating how the advantage
of MAIC methods over Bucher increases as overlap decreases.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-overlap-1} 

}

\caption{Distribution of bias by population overlap level. The dashed line indicates zero bias.}\label{fig:fig-overlap}
\end{figure}

\subsection{Effect of Effect
Modifiers}\label{effect-of-effect-modifiers}

The presence and number of effect modifiers was the primary determinant
of Bucher method bias. Table 3 shows that when no effect modifiers were
present, the Bucher method had minimal bias (-0.017) and MAIC methods
provided little advantage while increasing variance. With one effect
modifier, Bucher bias increased nearly 10-fold to -0.167, while MAIC
methods maintained bias around -0.034. With two effect modifiers, Bucher
bias reached -0.206 while MAIC methods remained at -0.032 to -0.042.

\begin{table}[h]
\centering
\caption{Bias by number of effect modifiers}
\begin{tabular}{lccc}
\hline
Method & No EM & One EM & Two EM \\
\hline
EbTotal/SigTotal & -0.025 & -0.034 & -0.032 \\
EbArm & -0.039 & -0.062 & -0.039 \\
EbEL & -0.023 & -0.044 & -0.042 \\
Bucher & -0.017 & -0.167 & -0.206 \\
\hline
\end{tabular}
\end{table}

Figure 3 displays the distribution of bias by effect modifier count,
clearly illustrating the divergence between Bucher and MAIC methods as
effect modification increases.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-em-1} 

}

\caption{Distribution of bias by number of effect modifiers.}\label{fig:fig-em}
\end{figure}

\subsection{Effective Sample Size}\label{effective-sample-size}

The effective sample size provides a diagnostic for weight concentration
and potential positivity violations. Figure 4 displays ESS distributions
by method. The Bucher method by definition has ESS equal to the original
sample size (300 average across scenarios). EbTotal/SigTotal showed mean
ESS of 192 (64\% retention). EbArm had similar ESS (186). EbEL showed
substantially lower ESS (147, 49\% retention), indicating more extreme
weight distributions.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-ess-1} 

}

\caption{Effective sample size distribution by method. Higher ESS indicates less extreme weighting.}\label{fig:fig-ess}
\end{figure}

Figure 5 displays the ESS by population overlap, showing the dramatic
ESS reduction under low overlap conditions. Under low overlap, EbEL
retained only 8\% of the original sample size on average (ESS = 24),
while EbTotal/SigTotal retained 29\% (ESS = 86).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-ess-overlap-1} 

}

\caption{Effective sample size by population overlap level.}\label{fig:fig-ess-overlap}
\end{figure}

\subsection{Coverage Probability}\label{coverage-probability}

Figure 6 displays 95\% confidence interval coverage by method.
EbTotal/SigTotal and EbArm achieved approximately nominal coverage
(94.3\%). The Bucher method (91.6\%) and EbEL (91.8\%) showed slight
undercoverage, though this may be attributable to bias rather than
variance underestimation.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-coverage-1} 

}

\caption{95\% confidence interval coverage by method. The dashed red line indicates nominal 95\% coverage.}\label{fig:fig-coverage}
\end{figure}

\subsection{Bias-ESS Tradeoff}\label{bias-ess-tradeoff}

Figure 7 illustrates the fundamental tradeoff between bias reduction and
effective sample size. The Bucher method achieves the highest ESS (no
weighting) but at the cost of substantial bias. MAIC methods reduce bias
by weighting, which necessarily reduces ESS. Among the MAIC methods,
EbTotal/SigTotal achieved the best balance, with the lowest bias and
highest ESS. EbEL showed lower ESS without compensating bias reduction.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-tradeoff-1} 

}

\caption{Bias-ESS tradeoff by method. Error bars show 95\% confidence intervals.}\label{fig:fig-tradeoff}
\end{figure}

\subsection{Scenario-Specific Results}\label{scenario-specific-results}

Figure 8 presents a heatmap of mean bias across all scenario
configurations. The pattern clearly shows that Bucher method bias
increases with decreasing overlap and increasing effect modification,
while MAIC methods maintain relatively consistent low bias across
conditions. The EbArm method showed elevated bias in some low-overlap
scenarios.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/fig-heatmap-1} 

}

\caption{Heatmap of mean bias across scenario configurations. Each cell shows the mean bias for a specific overlap/effect modifier combination.}\label{fig:fig-heatmap}
\end{figure}

\section{Discussion}\label{discussion}

This comprehensive simulation study provides several key insights for
MAIC methodology and practice.

\subsection{Equivalence of Method of Moments and Entropy
Balancing}\label{equivalence-of-method-of-moments-and-entropy-balancing-1}

The empirical confirmation of the theoretical equivalence between method
of moments and entropy balancing is a central finding of this study.
Across 270,000 simulation iterations, the two approaches produced
numerically identical point estimates to within machine precision. This
equivalence, first proven theoretically by Phillippo et al. (Phillippo
et al. 2020), has important practical implications. Practitioners need
not choose between the two approaches on statistical grounds, as they
yield identical results. However, the entropy balancing formulation
offers conceptual and computational advantages: it naturally extends to
non-uniform base weights, clearly specifies the objective being
optimized, and connects MAIC to the broader literature on covariate
balancing in causal inference.

\subsection{Value of Population Adjustment When Effect Modification is
Present}\label{value-of-population-adjustment-when-effect-modification-is-present}

The simulation results clearly demonstrate that MAIC methods
substantially reduce bias when effect modifiers are present and
populations differ. The unadjusted Bucher method showed 5 to 10-fold
higher bias than MAIC methods when one or two effect modifiers were
included in the data-generating model. This finding reinforces the
importance of population adjustment in indirect comparisons when
cross-trial differences in effect modifiers are suspected.

Conversely, when no effect modifiers were present, MAIC provided minimal
bias reduction while increasing variance through weighting. This finding
suggests that the decision to apply MAIC should be guided by prior
knowledge or evidence regarding effect modification. When effect
modification is unlikely, the unadjusted Bucher method may be preferred
to avoid unnecessary variance inflation.

\subsection{Limited Advantage of Alternative Loss
Functions}\label{limited-advantage-of-alternative-loss-functions}

The empirical likelihood method (EbEL), despite its theoretical appeal
based on the work of Owen (Owen 2001), did not demonstrate advantages
over standard entropy balancing in this simulation. EbEL showed higher
RMSE and substantially lower ESS without compensating bias reduction.
The lower ESS indicates more extreme weight distributions, which may
increase sensitivity to model misspecification and outliers. These
results suggest that standard entropy (Kullback-Leibler divergence)
remains the preferred loss function for MAIC applications.

\subsection{Conditional Utility of Arm-Separate
Weighting}\label{conditional-utility-of-arm-separate-weighting}

The arm-separate entropy balancing method (EbArm) showed mixed
performance. Under high and medium population overlap, EbArm performed
comparably to total-population methods. However, under low overlap,
EbArm exhibited substantial instability with RMSE of 2.30, nearly five
times higher than EbTotal. This instability likely arises because
arm-separate weighting effectively halves the sample size available for
each weighting problem, exacerbating the challenges of extreme weighting
under low overlap conditions.

These findings suggest that arm-separate weighting should be reserved
for scenarios where arm-specific aggregate data are available and
population overlap is adequate, ensuring sufficient effective sample
size within each arm. A practical guideline might be to require ESS
exceeding 50 within each arm before applying arm-separate methods.

\subsection{Importance of Effective Sample Size
Monitoring}\label{importance-of-effective-sample-size-monitoring}

Effective sample size emerged as a critical diagnostic for MAIC
analyses. Under low overlap conditions, ESS dropped dramatically, with
EbEL retaining only 8\% of the original sample size on average. Such
extreme ESS reduction indicates that only a small fraction of the IPD
contributes meaningfully to the weighted analysis, potentially leading
to high variance, sensitivity to outliers, and violations of the
positivity assumption.

Practitioners should routinely report ESS alongside treatment effect
estimates. An ESS below 40\% of the original sample size may warrant
concern about the reliability of the population adjustment. When ESS is
very low, consideration should be given to whether the target population
defined by the aggregate data is feasible given the index trial
population.

\subsection{Practical Recommendations}\label{practical-recommendations}

Based on these findings, we offer the following recommendations for MAIC
practice. First, entropy balancing (EbTotal) or the equivalent method of
moments (SigTotal) should be used as the default approach, as these
methods demonstrated the best balance of bias reduction and stability
across conditions. Second, the decision to apply MAIC should be informed
by prior evidence regarding effect modification; when effect
modification is unlikely, unadjusted indirect comparison may be
preferred. Third, effective sample size should be monitored as a
diagnostic for extreme weighting, with ESS below 40\% of the original
sample warranting caution. Fourth, arm-separate methods should be
reserved for high-overlap scenarios with adequate within-arm ESS. Fifth,
alternative loss functions such as empirical likelihood do not appear to
offer advantages over standard entropy balancing.

\subsection{Limitations}\label{limitations}

Several limitations should be considered when interpreting these
results. The simulation used logistic models for binary outcomes;
different patterns may emerge with continuous or time-to-event outcomes.
The comparator trial sample size was fixed at 500; smaller aggregate
data samples may introduce additional variability. The simulation did
not evaluate robustness to unmeasured effect modifiers, a fundamental
assumption of MAIC that cannot be verified from data. Finally, the
simulation used known treatment effects and covariate distributions;
real applications face uncertainty about these quantities.

\section{Conclusions}\label{conclusions}

This simulation study confirms the mathematical equivalence of method of
moments and entropy balancing MAIC and provides evidence-based guidance
for method selection. Standard entropy balancing is recommended as the
first-choice approach for population-adjusted indirect comparisons.
Alternative loss functions and arm-separate weighting do not offer clear
advantages and may perform worse under challenging conditions. Effective
sample size monitoring is essential for identifying potential problems
with extreme weighting. Future research should evaluate the robustness
of MAIC methods to unmeasured confounding and extend these comparisons
to time-to-event outcomes common in oncology applications.

\section{Data Availability}\label{data-availability}

The advmaic R package implementing all methods, simulation code, and
complete results are available at
\url{https://github.com/choxos/advmaic}. Supplementary materials
including full scenario-level results tables and additional figures are
available in the simulation/results directory of the repository.

\section{Acknowledgments}\label{acknowledgments}

The author thanks the developers of the R packages used in this study,
particularly dplyr, ggplot2, sandwich, and nloptr.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\protect\phantomsection\label{refs}
\begin{CSLReferences}{1}{1}
\bibitem[\citeproctext]{ref-bucher1997}
Bucher, Heiner C, Gordon H Guyatt, Lauren E Griffith, and Stephen D
Walter. 1997. {``The Results of Direct and Indirect Treatment
Comparisons in Meta-Analysis of Randomized Controlled Trials.''}
\emph{Journal of Clinical Epidemiology} 50 (6): 683--91.

\bibitem[\citeproctext]{ref-cressie1984}
Cressie, Noel, and Timothy RC Read. 1984. {``Multinomial Goodness-of-Fit
Tests.''} \emph{Journal of the Royal Statistical Society: Series B
(Methodological)} 46 (3): 440--64.

\bibitem[\citeproctext]{ref-owen2001}
Owen, Art B. 2001. \emph{Empirical Likelihood}. Chapman; Hall/CRC.

\bibitem[\citeproctext]{ref-petto2019}
Petto, Hwee, Zbigniew Kadziola, Alan Brnabic, et al. 2019.
{``Alternative Weighting Approaches for Anchored Matching-Adjusted
Indirect Comparisons via a Common Comparator.''} \emph{Value in Health}
22 (1): 85--91.

\bibitem[\citeproctext]{ref-phillippo2020}
Phillippo, David M, Sofia Dias, A E Ades, and Nicky J Welton. 2020.
{``Equivalence of Entropy Balancing and the Method of Moments for
Matching-Adjusted Indirect Comparison.''} \emph{Research Synthesis
Methods} 11 (4): 568--72.

\bibitem[\citeproctext]{ref-signorovitch2012}
Signorovitch, James E, Vanja Sikirica, M Haim Erder, et al. 2012.
{``Matching-Adjusted Indirect Comparisons: A New Tool for Timely
Comparative Effectiveness Research.''} \emph{Value in Health} 15 (6):
940--47.

\bibitem[\citeproctext]{ref-signorovitch2010}
Signorovitch, James E, Eric Q Wu, Andy P Yu, et al. 2010. {``Comparative
Effectiveness Without Head-to-Head Trials: A Method for
Matching-Adjusted Indirect Comparisons Applied to Psoriasis Treatment
with Adalimumab or Etanercept.''} \emph{PharmacoEconomics} 28 (10):
935--45.

\bibitem[\citeproctext]{ref-williamson2014}
Williamson, Elizabeth J, Andrew Forbes, and Ian R White. 2014.
{``Variance Reduction in Randomised Trials by Inverse Probability
Weighting Using the Propensity Score.''} \emph{Statistics in Medicine}
33 (5): 721--37.

\end{CSLReferences}

\end{document}
